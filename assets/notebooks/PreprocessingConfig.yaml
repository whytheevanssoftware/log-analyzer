UnsupervisedGlobalConfig:
    embed_size: 512
    max_vocab_size: 30000
    buffer_size: 10000
    global_training: True
    path: '/results/'

PreprocessingGlobalConfig:    
    embed_size: 768
    max_vocab_size: 30522
    buffer_size: 10000
    global_training: True
    num_neg_sampling: 10
    path: '/results/'
    
PhraserModelConfig:
    min_count: 5
    threshold: 7
    load_model: False
    save_model: True
    training: False
    model_name: 'phrase_model.joblib'
    
TextClusteringConfig:
    load_model: False
    save_model: True
    training: False
    model_name: 'template_miner.joblib'
    
WordEmbeddingPreTrainingConfig:
    # BERT
    hidden_size: 512
    num_hidden_layers: 8
    num_attention_heads: 8
        
    # Save Paths
    pretrained_save_path: "../results/PreTrainedModel/"
    checkpoint_save_path: "../results/PreTrainedChkpts/"
    checkpoint_max_saves: 3
    checkpoint_save_interval: 200
    
    # Learning rate schedule
    init_lr: 0.0001
    num_warmup_steps: 2
    
    # Optimizer
    adam_beta1: 0.9
    adam_beta2: 0.999
    adam_epsilon: 0.1
    weight_decay_rate: 0.1
    
    epochs: 2
    batch_size: 10
    max_seq_length: 300
    repeat_size: 5
        
    # State 
    load_model: True
    save_model: False
    training: True
    model_name: 'BERT'